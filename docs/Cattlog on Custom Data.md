# Cattlog and ID on custom data

Author: Manu Ramesh
---

Follow these steps to train and run the AutoCattlogger and AutoCattleID on custom data. 

## Steps
- **Data Collection and annotation:** Collect a few sample images of cows from your data (around 20-30), and annotate them with cow masks and 10 keypoints per cow. You can follow instructions **in this video** for help with annotations.

- **Training detectors:** Append our dataset of nearly a 1000 images with your data samples and train the mask (maskRCNN) and keypoint (HRNet) detector models following instructions in the [readme in the dependecies folder](../dependencies/readme.md).

- **Obtain shape statistics:** Obtain the cow-shape statistics (bounds for rules checker2) by running the [computeLimitsForRulesChecker](../autoCattlogger/tools/computeLimitsForRulesChecker.py) as per instructions in the [tools readme file](../autoCattlogger/tools/readme.md) from your new keypoints dataset and save it in the [models directory](../models/keypointStat_models/). You can skip this step if your data looks similar to our data.

- **Create AutoCattlogger config:** Create a config file for the AutoCattlogger using the [supplied example](../configs/autoCattlogger_configs/autoCattlogger_example_config.yaml) as a guide.

- **Procure ground-truth cow-labels:** Create a CSV file with a list of cowIDs (ear-tag numbers) in the same order in which the cows appear in the scene. You can skip and revisit this step in case you are unable to determine the perfect order in which the cows have walked. 

- **Arrange your top-view cow-videos:** Arrange all videos of cows on which you wish to run the AutoCattlogger into a folder. Make sure to name the videos alphabetically so they sort correctly for the AutoCattlgger to process them in the correct order.

- **Run the AutoCattlogger on the videos:** Follow instructions in the [main readme file](../README.md) to generate the cattlog and track outputs. If you do not have the ground-truth labels csv file, you can use the initial autoCattlogger video outputs to help you find the correct order of apperance of cows. You can rerun the cattlog generation code with the -g option to attach the ground-truth labels to the already generated cattlog (this will not reprocess the videos). 

- **Create the scene illumination map:** If necessary and if you have a completely black cow in your herd, you could use it to genrate the scene illumination map using instructions in the [bmlp readme file](../autoCattlogger/bmlp/readme.md). The estimated scene illumination map (the extended illumination map) can be used to generate highger quality barcodes by eliminating the effects of specular highlights on black regions of cows. 

- **Re-run the AutoCattlogger with color-correction:** Modify the AutoCattlogger config to set colorCorrectionOn value to True, and supply path to the saved illumination map image under the blackPointImgPth key. 
Re-run the autoCattlogger using this new config. This should generate color-corrected video frames and higher quality barcodes for all track-points in all cow-tracks. 

- **Identify cows:** Follow the instructions in the [main readme file](../README.md) to use the [inference script](../inferID.py) to identify cows from images, videos, or a set of videos in a directory. 

- **Evaluation:** To evaluate the identification system performance, you must first generate the tracks from another set of videos using the AutoCattlogger by following the above instructions. Then, follow the instructions in the [main readme file](../README.md) to run the [evaluation script](../evaluateAutoCattleID.py). 
Note that it is best to use barcodes generated with color-correction on -- for both the training tracks and the evaluation tracks as these will be of higher quality.

## Tips
- You might have to set the frame-width and frame-height values if you are not using 1920x1080 resolution videos in some cases such as track-point filtering.
- It is best to start with the sample scripts in the [main directory](../) and then call the autoCattlogger functions in your own python files once you get comfortable with the repository.
- We expect the bodies of cows to look rectangular in the top view and expect them to have a certain area with respect to the area of the frame. So, the AutoCattlogger desk rejects cows that do not fall within a set aspect ratio and area ratio bounds and does not process them further. You can observe this rejection/skipping in the log file or the output videos generated by the autoCattlogger. If an instance has 'AR' followed by a value (aspect ratio value) in the output video, it means that the instance was rejcted because it's aspect ratio or area ratio was out of the allowed bounds.
    -  In case your data looks very different from our data, you can try to modify these bounds in the [autoCattlogger config file](../configs/autoCattlogger_configs/autoCattlogger_example_config.yaml).
    - cowAspRatio_ulim: Stands for cow aspect ratio upper limit. It is the ratio of widht to height. AutoCattlogger automatically recomputes the bounds if the cow appears vertical instad of horizontal.
    - cowAreaRatio_llim: Stands for cow area ratio lower limit. It is the area of the cow's oriented bounding box to the area of the video frame. Set this to low values if the cows appear smaller in your videos than in ours. Use the logs as reference to get an idea of what a good limit would be.